{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spltting up the data into smaller chunks for it to be accessed\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "SRC = Path(\"/content/drive/MyDrive/FFHQ\")\n",
    "DEST = Path(\"/content/drive/MyDrive/FFHQ_SPLIT\")\n",
    "DEST.mkdir(exist_ok=True)\n",
    "\n",
    "IMAGES_PER_FOLDER = 3000\n",
    "\n",
    "files = sorted([p for p in SRC.iterdir() if p.suffix.lower() in [\".png\",\".jpg\",\".jpeg\"]])\n",
    "\n",
    "folder_index = 1\n",
    "count = 0\n",
    "\n",
    "subfolder = DEST / f\"part_{folder_index}\"\n",
    "subfolder.mkdir(exist_ok=True)\n",
    "\n",
    "for img in files:\n",
    "    if count >= IMAGES_PER_FOLDER:\n",
    "        folder_index += 1\n",
    "        count = 0\n",
    "        subfolder = DEST / f\"part_{folder_index}\"\n",
    "        subfolder.mkdir(exist_ok=True)\n",
    "\n",
    "    shutil.move(str(img), str(subfolder / img.name))\n",
    "    count += 1\n",
    "    if count % 500 == 0:\n",
    "        print(f\"Moved {count} images into {subfolder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "from torchvision import transforms\n",
    "import gc\n",
    "import signal\n",
    "\n",
    "#timeout exception to deal with files that take too long to load\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException()\n",
    "\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "#configuration\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "FFHQ_ROOT = Path(\"/content/drive/MyDrive/FFHQ_SPLIT\")\n",
    "OUTPUT_DIR = Path(\"/content/drive/MyDrive/HairCLIP\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 5000\n",
    "HIGH_CONF_THRESHOLD = 0.3\n",
    "MED_CONF_THRESHOLD = 0.1\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "HAIRSTYLES = [\n",
    "    \"buzz cut hairstyle\", \"fade hairstyle\", \"high fade hairstyle\",\n",
    "    \"low fade hairstyle\", \"taper fade hairstyle\", \"middle part hairstyle\",\n",
    "    \"side part hairstyle\", \"wolfcut hairstyle\", \"mullet hairstyle\",\n",
    "    \"bob cut hairstyle\", \"pixie cut hairstyle\", \"ponytail hairstyle\",\n",
    "    \"long curly hairstyle\", \"long straight hairstyle\", \"short curly hairstyle\",\n",
    "    \"short straight hairstyle\", \"bowl cut hairstyle\", \"undercut hairstyle\",\n",
    "    \"textured crop hairstyle\", \"modern mullet hairstyle\", \"curtained flow hairstyle\",\n",
    "    \"hockey flow hairstyle\", \"butterfly cut hairstyle\", \"curtain bangs hairstyle\",\n",
    "    \"short length hairstyle\", \"medium length hairstyle\", \"long length hairstyle\", \"dreads hairstyle\",\n",
    "    \"braids hairstyle\"\n",
    "]\n",
    "\n",
    "#loading clip model onto device\n",
    "print(f\"Loading CLIP model on {DEVICE}...\")\n",
    "clip_model, preprocess_clip = clip.load(\"ViT-B/32\", device=DEVICE)\n",
    "\n",
    "prompts = [f\"A photo of a person with a {h}.\" for h in HAIRSTYLES]\n",
    "text_tokens = clip.tokenize(prompts).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.encode_text(text_tokens)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#finding all images to process\n",
    "def scan_images_recursively(root):\n",
    "    \"\"\"Walk subfolders and return list of full paths.\"\"\"\n",
    "    images = []\n",
    "    for root_dir, dirs, files in os.walk(root):\n",
    "        for fname in files:\n",
    "            if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                images.append(os.path.join(root_dir, fname))\n",
    "    return images\n",
    "\n",
    "print(\"Scanning FFHQ_SPLIT recursively...\")\n",
    "all_images = scan_images_recursively(FFHQ_ROOT)\n",
    "print(f\"Found {len(all_images)} images across subfolders.\")\n",
    "\n",
    "num_batches = (len(all_images) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "print(f\"Processing in {num_batches} batches...\")\n",
    "\n",
    "\n",
    "def clip_label_image(img_path):\n",
    "    img = preprocess_clip(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        img_feat = clip_model.encode_image(img)\n",
    "        img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "        sims = (img_feat @ text_features.T).squeeze(0)\n",
    "        best_idx = sims.argmax().item()\n",
    "        score = sims[best_idx].item()\n",
    "        return HAIRSTYLES[best_idx], score\n",
    "\n",
    "#Use zero shot with CLIP model to assign images a text embedding\n",
    "for batch_idx in range(num_batches):\n",
    "    batch_images = all_images[batch_idx*BATCH_SIZE : (batch_idx+1)*BATCH_SIZE]\n",
    "    batch_labels = {}\n",
    "\n",
    "    print(f\"\\nProcessing batch {batch_idx+1}/{num_batches} ({len(batch_images)} images)...\")\n",
    "\n",
    "    for img_i, img_path in enumerate(batch_images):\n",
    "\n",
    "        if img_i % 200 == 0:\n",
    "            print(f\"  Processed {img_i}/{len(batch_images)} images...\", flush=True)\n",
    "\n",
    "        try:\n",
    "            signal.alarm(5)\n",
    "            label, score = clip_label_image(img_path)\n",
    "            signal.alarm(0)\n",
    "\n",
    "            if score >= MED_CONF_THRESHOLD:\n",
    "                batch_labels[os.path.basename(img_path)] = {\n",
    "                    \"label\": label,\n",
    "                    \"confidence\": float(score),\n",
    "                    \"use_for_training\": score >= HIGH_CONF_THRESHOLD\n",
    "                }\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(f\"⏳ Timeout on {img_path}, skipping.\")\n",
    "            signal.alarm(0)\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {img_path}: {e}\")\n",
    "            signal.alarm(0)\n",
    "            continue\n",
    "\n",
    "    batch_file = OUTPUT_DIR / f\"ffhq_labels_batch_{batch_idx}.json\"\n",
    "    with open(batch_file, \"w\") as f:\n",
    "        json.dump(batch_labels, f, indent=4)\n",
    "\n",
    "    print(f\"Saved batch labels to {batch_file}\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"\\nMerging all batch JSONs...\")\n",
    "final_labels = {}\n",
    "\n",
    "batch_files = sorted(OUTPUT_DIR.glob(\"ffhq_labels_batch_*.json\"))\n",
    "if not batch_files:\n",
    "    raise ValueError(\"No batch JSON files found!\")\n",
    "\n",
    "for batch_file in batch_files:\n",
    "    with open(batch_file, \"r\") as f:\n",
    "        final_labels.update(json.load(f))\n",
    "\n",
    "final_file = OUTPUT_DIR / \"ffhq_labels.json\"\n",
    "with open(final_file, \"w\") as f:\n",
    "    json.dump(final_labels, f, indent=4)\n",
    "\n",
    "print(f\"All batches merged. Final labels saved to {final_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
